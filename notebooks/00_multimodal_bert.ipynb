{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0MHWWuLRcvP-"},"source":["# Multimodal BERT\n","\n","- From scratch code from KAIST Pr4AI - ai504_12_bert_sol.ipynb\n","- Remove embedding matrix, vocab etc."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"igesORsjJ3wf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import LayerNorm\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.optim import Adam\n","\n","import math\n","import tqdm\n","import random\n","import pickle\n","from collections import Counter\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Resources\n","\n","- https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n","- https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch\n"]},{"cell_type":"markdown","metadata":{"id":"N3sj5DfJ0ccp"},"source":["# Transformer Dims Math"]},{"cell_type":"markdown","metadata":{},"source":["### # of attention heads & hidden width\n","- `hidden` or `d_model` mod `att_heads` must be 0, i.e. `d_model` must be a multiple of `# of heads`\n","- `hidden % attn_heads (12) = 0`\n","- `hidden` or `d_model` is the width of the former embedding vector. Each token will go into the Transformer as vector of this width."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GslqAKyI4dJ7"},"outputs":[{"data":{"text/plain":["(384, 768, 1536)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["12 * 32, 12 * 64, 12 * 128      # 12 heads and hidden size as multiples of 32, 64, 128\n","#GPT-1 = 12 layers and 12 heads, 64 dimensional states = 768 hidden size"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["(192, 384, 768)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["6 * 32, 6 * 64, 6 * 128        # 6 heads and hidden size as multiples of 32, 64, 128"]},{"cell_type":"markdown","metadata":{},"source":["- A typical language model has `seq_len` (# of words in each inout) of 512\n","- So typical sizes for each input in batch would be:\n","    - for 512 tokens each 768 wide - input to Transformer = `512 * 768 = 393,216` \n","    - for 256 tokens each 768 wide - `512 * 768 = 196,608` \n","- Overall input dim = `bs x seq_len x emb_width` "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RPD6nPCx3A26"},"source":["## BERT From Scratch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6XO4P1Ye4VF8"},"outputs":[],"source":["class Attention(nn.Module):\n","    \"\"\"\n","    Compute 'Scaled Dot Product Attention\n","    \"\"\"\n","\n","    def forward(self, query, key, value, mask=None, dropout=None):                         # query, key, value: (B, h, seq_len, d_k) eg.(B, 12, seq_len, 64)\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))    # torch.transpose(input, dim0, dim1): The given dimensions dim0 and dim1 are swapped.\n","                                                                                           # scores: (B, h, seq_len, seq_len) \n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9)                        # masked_fill(mask, value): Fills elements of self tensor with value where mask is True. \n","                                                                               \n","        p_attn = F.softmax(scores, dim=-1)                                      # p_attn: (B, h, seq_len, seq_len) \n","\n","        if dropout is not None:\n","            p_attn = dropout(p_attn)\n","\n","        return torch.matmul(p_attn, value), p_attn                              # torch.matmul(p_attn, value): (B, h, seq_len, d_k), p_attn: (B, h, seq_len, seq_len)\n","                                                                                \n","\n","class MultiHeadedAttention(nn.Module):\n","    \"\"\"\n","    Take in model size and number of heads.(d_model, h)\n","    \"\"\"\n","\n","    def __init__(self, h, d_model, dropout=0.1):\n","        super().__init__()\n","        assert d_model % h == 0                                                 \n","\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h                                                 # eg. d_model = 768, h = 12, d_k = 64\n","        self.h = h\n","\n","        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])  # 3 linear layer. WQ, WK, WV   eg.(768, 768(64 x 12))\n","        self.output_linear = nn.Linear(d_model, d_model)                        # 768->768\n","        self.attention = Attention()\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, query, key, value, mask=None):                            # input query, key, value will be x with (B, seq_len, d_model). \n","        batch_size = query.size(0)\n","\n","        # 1) Do all the linear projections in batch from d_model -> h x d_k     eg. 768 -> (12, 64)\n","        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)  # l(x): (B, seq_len, d_model) -> (B, seq_len, h, d_k) -> (B, h, seq_len, d_k) \n","                             for l, x in zip(self.linear_layers, (query, key, value))]    # output: (B, h, seq_len, d_k)  eg.(B, 12, seq_len, 64)\n","\n","        # 2) Apply attention on all the projected vectors in batch.\n","        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)     # x:(B, h, N, d_k) eg.(B, 12, seq_len, 64), attn:(B, h, seq_len, seq_len) eg.(B, 12, seq_len, seq_len)\n","\n","        # 3) \"Concat\" using a view and apply a final linear.\n","        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)    # x:(B, h, seq_len, d_k) -> (B, seq_len, h, d_k)  -> (B, seq_len, h * d_k)  eg.(B, seq_len, 768)\n","                                                                                      # When you call contiguous(), it actually makes a copy of tensor so the order of elements would be same as if tensor of same shape created from scratch. Normally you don't need to worry about this. \n","                                                                                      # https://stackoverflow.com/questions/48915810/pytorch-contiguous\n","        return self.output_linear(x)                                                  # final output: (B ,seq_len, d_model) eg.(B, seq_len, 768) "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"HqDk0Jnh9IAL"},"outputs":[],"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):                                          #  size :  eg. 768\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):                                             # the input 'sublayer' will be Multihead attention or FF\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))                         "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4krr9efo9IJA"},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)                                     # d_ff: d_model x 4,  eg. 768 x 4 = 3072\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.GELU()\n","\n","    def forward(self, x):                                                       # x: (B, seq_len, d_model)\n","        return self.w_2(self.dropout(self.activation(self.w_1(x))))             # (B, seq_len, d_model)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vl9YhzDZ3LXQ"},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    \"\"\"\n","    Bidirectional Encoder = Transformer (self-attention)\n","    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n","    \"\"\"\n","\n","    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n","        \"\"\"\n","        :param hidden: hidden size of transformer\n","        :param attn_heads: head sizes of multi-head attention\n","        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n","        :param dropout: dropout rate\n","        \"\"\"\n","\n","        super().__init__()\n","        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n","        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n","        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n","        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x, mask):           # x: (B, seq_len, hidden)   eg.(B, seq_len, 768)\n","        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))    \n","        x = self.output_sublayer(x, self.feed_forward)                                            \n","        return self.dropout(x)            # (B, seq_len, hidden)      "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Old NLP BERT - Delete"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nz8qtonX36o5"},"source":["```python\n","class TokenEmbedding(nn.Embedding):                                             # torch.nn.Embedding(vocab_size, embedding_dim, padding_idx)\n","    def __init__(self, vocab_size, embed_size=512):                             # padding_idx=0 means 0th row in the embeding matrix is 0 vector.\n","        super().__init__(vocab_size, embed_size, padding_idx=0)\n","\n","\n","class PositionalEmbedding(nn.Module):\n","\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model).float()                              # pe: (max_len, d_model)\n","        pe.require_grad = False\n","\n","        position = torch.arange(0, max_len).float().unsqueeze(1)                                  # (max_len, 1)  eg. (512, 1)\n","        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()   # (d_model//2,) eg. (384,)\n","        a = position * div_term                                                                   # (max_len, d_model//2)  eg. (512, 384)\n","        pe[:, 0::2] = torch.sin(a)                            # seq = L[start:stop:step]    0::2 start 0, step_size = 2  \n","        pe[:, 1::2] = torch.cos(a)                            \n","\n","        pe = pe.unsqueeze(0)                                                    # pe: (1, max_len, d_model)\n","        self.register_buffer('pe', pe)                                          # Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\n","                                                                                # register_buffer(name, tensor)\n","    def forward(self, x):                         # x: (B, seq_len)\n","        return self.pe[:, :x.size(1)]             # (1, seq_len, d_model)\n","\n","\n","class SegmentEmbedding(nn.Embedding):\n","    def __init__(self, embed_size=512):\n","        super().__init__(3, embed_size, padding_idx=0)                          #0(padding), 1(first sentence), 2(second sentence)\n","\n","\n","class BERTEmbedding(nn.Module):\n","    \"\"\"\n","    BERT Embedding which is consisted with under features\n","        1. TokenEmbedding : normal embedding matrix\n","        2. PositionalEmbedding : adding positional information using sin, cos\n","        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n","        sum of all these features are output of BERTEmbedding\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, embed_size, dropout=0.1):                    # embed_size: eg. 768     \n","        \"\"\"\n","        :param vocab_size: total vocab size\n","        :param embed_size: embedding size of token embedding\n","        :param dropout: dropout rate\n","        \"\"\"\n","        super().__init__()\n","        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n","        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n","        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embed_size = embed_size\n","\n","    def forward(self, sequence, segment_label):                                             # sequence: (B, seq_len), segment_label: (B, seq_len)\n","        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)    # (B, seq_len, hidden), (1, seq_len, hidden), (B, seq_len, hidden), respectively. Broadcasting;  \n","        return self.dropout(x)\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","```python\n","class BERT(nn.Module):\n","    \"\"\"\n","    BERT model : Bidirectional Encoder Representations from Transformers.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n","        \"\"\"\n","        :param vocab_size: vocab_size of total words\n","        :param hidden: BERT model hidden size\n","        :param n_layers: numbers of Transformer blocks(layers)\n","        :param attn_heads: number of attention heads\n","        :param dropout: dropout rate\n","        \"\"\"\n","\n","        super().__init__()\n","        self.hidden = hidden\n","        self.n_layers = n_layers\n","        self.attn_heads = attn_heads\n","\n","        # paper noted they used 4*hidden_size for ff_network_hidden_size\n","        self.feed_forward_hidden = hidden * 4\n","\n","        # embedding for BERT, sum of positional, segment, token embeddings\n","        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n","\n","        # multi-layers transformer blocks, deep network\n","        print(f\"h (attn_heads): {attn_heads}, d_model (hidden): {hidden}, d_model%h = {hidden%attn_heads}\")\n","        self.transformer_blocks = nn.ModuleList(\n","            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n","\n","    def forward(self, x, segment_info, skip_emb=False):                                         # x: (B, seq_len),      segment_info: (B, seq_len)\n","        \n","        # attention masking for padded token\n","        # attention mask size: torch.ByteTensor([batch_size, 1, seq_len, seq_len])  Why? Because of 'scores' shape in Attention class: (B, h, seq_len, seq_len)\n","        # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)        # (B, seq_len) -> (B, 1, seq_len) -> (B, seq_len, seq_len) -> (B, 1, seq_len, seq_len)\n","\n","        # embedding the indexed sequence to sequence of vectors\n","        if not skip_emb:\n","            x = self.embedding(x, segment_info)\n","\n","        print('x.shape: ', x.shape)\n","        # running over multiple transformer blocks\n","        for transformer in self.transformer_blocks:\n","            x = transformer.forward(x, mask=None)\n","\n","        return x\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g1VBXXsljhru"},"source":["### BERT From Scratch (without embedding)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Y6euB6S9xufI"},"outputs":[],"source":["class BERTFromScratch(nn.Module):\n","    \"\"\"\n","    BERT model : Bidirectional Encoder Representations from Transformers.\n","    \"\"\"\n","\n","    def __init__(self, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n","        \"\"\"\n","        :param vocab_size: vocab_size of total words\n","        :param hidden: BERT model hidden size\n","        :param n_layers: numbers of Transformer blocks(layers)\n","        :param attn_heads: number of attention heads\n","        :param dropout: dropout rate\n","        \"\"\"\n","\n","        super().__init__()\n","        self.hidden = hidden\n","        self.n_layers = n_layers\n","        self.attn_heads = attn_heads\n","\n","        # paper noted they used 4*hidden_size for ff_network_hidden_size\n","        self.feed_forward_hidden = hidden * 4\n","\n","        # embedding for BERT, sum of positional, segment, token embeddings\n","        # self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n","\n","        # multi-layers transformer blocks, deep network\n","        print(f\"h (attn_heads): {attn_heads}, d_model (hidden): {hidden}, d_model%h = {hidden%attn_heads}\")\n","        self.transformer_blocks = nn.ModuleList(\n","            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n","\n","    def forward(self, x):                                         # x: (B, seq_len),      segment_info: (B, seq_len)\n","        \n","        # attention masking for padded token\n","        # attention mask size: torch.ByteTensor([batch_size, 1, seq_len, seq_len])  Why? Because of 'scores' shape in Attention class: (B, h, seq_len, seq_len)\n","        # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)        # (B, seq_len) -> (B, 1, seq_len) -> (B, seq_len, seq_len) -> (B, 1, seq_len, seq_len)\n","\n","        print(f'x.shape from BERT: {x.shape}')\n","        # running over multiple transformer blocks\n","        for transformer in self.transformer_blocks:\n","            x = transformer.forward(x, mask=None)\n","\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-ikJ5j64eSA","outputId":"cf9b6e0b-6a24-4f18-ca16-f55f7d1e921b"},"outputs":[{"name":"stdout","output_type":"stream","text":["h (attn_heads): 12, d_model (hidden): 768, d_model%h = 0\n","x.shape from BERT: torch.Size([2, 10, 768])\n","shape of input x: torch.Size([2, 10, 768])\n","shape of output y: torch.Size([2, 10, 768])\n"]}],"source":["bert = BERTFromScratch(hidden=768)\n","x = torch.randn(2, 10, 768)\n","# x = torch.tensor([[3., 19, 4, 2, 7, 7, 2, 0, 0, 0],               # (2, 10) 2=Batch_size, 10=seq_len \n","#                   [3, 8, 7, 4, 2, 5, 6, 8, 2, 0]])\n","# segment_info = torch.randint(0, 3, (2, 10, 768))\n","# segment_info = torch.tensor([[1, 1, 1, 1, 2, 2, 2, 0, 0, 0],     # (2, 10)          \n","#                              [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]])\n","y = bert(x)\n","print('shape of input x:',x.size())\n","print('shape of output y:', y.size())"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZ8e7BSH3eOg","outputId":"3c5c0681-75e8-4feb-adfb-cbaf9fdc91a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters: ,  85,054,464\n"]}],"source":["### Total Parameters\n","params = sum([p.nelement() for p in bert.parameters()])\n","print(f\"Total Parameters: , {params: ,}\")      # nelement(): Alias for numel() "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## BERT PyTorch (Not Scratch) (without embedding)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Create a BERT model class in PyTorch with 12 encoder layers, 12 attention heads, and hidden size 786\n","class BERT(nn.Module):\n","    def __init__(self, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n","        super(BERT, self).__init__()\n","        self.hidden = hidden\n","        self.n_layers = n_layers\n","        self.attn_heads = attn_heads\n","        self.dropout = dropout\n","\n","        # BERT consists of a stack of 12 identical encoder layers\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=hidden,\n","            nhead=attn_heads,\n","            dim_feedforward=hidden * 4,\n","            dropout=dropout,\n","            activation=\"gelu\",\n","            batch_first=True,\n","        )\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n","\n","    def forward(self, x):\n","        # x: [batch_size, seq_len, hidden]\n","\n","        print(f\"x.shape from BERT: {x.shape}\")\n","        x = self.encoder(x)\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Pretrained feature vectors to BERT input "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Realigning the vectors"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([2, 15])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x_test = torch.tensor([[0, 1., 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],           \n","                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]])\n","x_test.shape"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 0.,  1.,  2.,  3.,  4.],\n","         [ 5.,  6.,  7.,  8.,  9.],\n","         [10., 11., 12., 13., 14.]],\n","\n","        [[ 0.,  1.,  2.,  3.,  4.],\n","         [ 5.,  6.,  7.,  8.,  9.],\n","         [10., 11., 12., 13., 14.]]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# x_test.view(2, 3, 5)\n","x_test.view(2, 3, 5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### HAIM pretrained vectors to BERT input"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modality</th>\n","      <th>col</th>\n","      <th>width</th>\n","      <th>percentage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Demographics</td>\n","      <td>de</td>\n","      <td>6</td>\n","      <td>0.148478</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Chart events</td>\n","      <td>ts_ce</td>\n","      <td>99</td>\n","      <td>2.449889</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Lab events</td>\n","      <td>ts_le</td>\n","      <td>242</td>\n","      <td>5.988617</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Procedure events</td>\n","      <td>ts_pe</td>\n","      <td>110</td>\n","      <td>2.722098</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ECG notes</td>\n","      <td>n_ecg</td>\n","      <td>768</td>\n","      <td>19.005197</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Echo notes</td>\n","      <td>n_ech</td>\n","      <td>768</td>\n","      <td>19.005197</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>X-ray dense</td>\n","      <td>vd</td>\n","      <td>1024</td>\n","      <td>25.340262</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>X-ray multiple dense</td>\n","      <td>vmd</td>\n","      <td>1024</td>\n","      <td>25.340262</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Modality    col  width  percentage\n","0          Demographics     de      6    0.148478\n","1          Chart events  ts_ce     99    2.449889\n","2            Lab events  ts_le    242    5.988617\n","3      Procedure events  ts_pe    110    2.722098\n","4             ECG notes  n_ecg    768   19.005197\n","5            Echo notes  n_ech    768   19.005197\n","6           X-ray dense     vd   1024   25.340262\n","7  X-ray multiple dense    vmd   1024   25.340262"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["modalities = {\n","    'Modality': ['Demographics', 'Chart events', 'Lab events', 'Procedure events', 'ECG notes', 'Echo notes', 'X-ray dense', 'X-ray multiple dense'],\n","    'col': ['de', 'ts_ce', 'ts_le', 'ts_pe', 'n_ecg', 'n_ech', 'vd', 'vmd'],\n","    'width': [6, 99, 242, 110, 768, 768, 1024, 1024]\n","}\n","\n","modalities_df = pd.DataFrame(modalities)\n","modalities_df['percentage'] = (modalities_df.width / modalities_df.width.sum()) * 100\n","modalities_df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- HAIM pretrained vector width = **4,041**\n","- Number of modalities = **8**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Options**\n","1. Consider each modality as a single token and convert it (with individual FF layers) to `emb_width` say `768` or `1536`\n","    - This will result in `8 * 768 = 6144` or `8 * 1536 = 12,288` per input patient\n","    - Model only has a seq_len of 8\n","2. Proportionally scale each modality (with individual FF layers) to add up to a max of `512 * 768 = 393,216` or `768 * 256 = 196,608`\n","    - Model still only has a seq_len of 8 but the emb_widths add up to look like a normal Transformer\n","    - Model could be made to use 512 or 256 tokens but will that look similar to option 3?\n","3. Scale the 4,041 with a FFN to 393,216 or 196,608, then chop that vector into 512 or 256 \"tokens\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["small_x = pd.read_csv('../dataset/mimiciv/mit_pretrained/small_x.csv')\n","small_y = pd.read_csv('../dataset/mimiciv/mit_pretrained/small_y.csv')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>de_0</th>\n","      <th>de_1</th>\n","      <th>de_2</th>\n","      <th>de_3</th>\n","      <th>de_4</th>\n","      <th>de_5</th>\n","      <th>vd_0</th>\n","      <th>vd_1</th>\n","      <th>vd_2</th>\n","      <th>vd_3</th>\n","      <th>...</th>\n","      <th>n_ech_758</th>\n","      <th>n_ech_759</th>\n","      <th>n_ech_760</th>\n","      <th>n_ech_761</th>\n","      <th>n_ech_762</th>\n","      <th>n_ech_763</th>\n","      <th>n_ech_764</th>\n","      <th>n_ech_765</th>\n","      <th>n_ech_766</th>\n","      <th>n_ech_767</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>72.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.004280</td>\n","      <td>0.026188</td>\n","      <td>0.263595</td>\n","      <td>0.000158</td>\n","      <td>...</td>\n","      <td>0.008949</td>\n","      <td>-0.101050</td>\n","      <td>0.210530</td>\n","      <td>-0.131684</td>\n","      <td>-0.138715</td>\n","      <td>-0.245273</td>\n","      <td>0.038294</td>\n","      <td>0.984190</td>\n","      <td>-0.270763</td>\n","      <td>0.999857</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>82.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.012419</td>\n","      <td>0.003249</td>\n","      <td>0.116324</td>\n","      <td>0.000791</td>\n","      <td>...</td>\n","      <td>0.038696</td>\n","      <td>-0.130439</td>\n","      <td>0.102237</td>\n","      <td>0.025674</td>\n","      <td>-0.135884</td>\n","      <td>-0.178948</td>\n","      <td>-0.064230</td>\n","      <td>0.998583</td>\n","      <td>-0.202867</td>\n","      <td>0.999945</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>69.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.020264</td>\n","      <td>0.444775</td>\n","      <td>0.032252</td>\n","      <td>...</td>\n","      <td>0.039694</td>\n","      <td>-0.141062</td>\n","      <td>-0.005484</td>\n","      <td>-0.036408</td>\n","      <td>-0.018071</td>\n","      <td>-0.271337</td>\n","      <td>0.008223</td>\n","      <td>0.972547</td>\n","      <td>-0.268833</td>\n","      <td>0.999649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>70.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>0.080409</td>\n","      <td>0.463351</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.003432</td>\n","      <td>-0.111446</td>\n","      <td>0.232872</td>\n","      <td>-0.162072</td>\n","      <td>-0.080258</td>\n","      <td>-0.307298</td>\n","      <td>-0.025907</td>\n","      <td>0.960139</td>\n","      <td>-0.339089</td>\n","      <td>0.999677</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>73.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.086402</td>\n","      <td>0.225096</td>\n","      <td>0.003878</td>\n","      <td>...</td>\n","      <td>0.064002</td>\n","      <td>-0.164014</td>\n","      <td>-0.018618</td>\n","      <td>-0.031740</td>\n","      <td>-0.092928</td>\n","      <td>-0.242412</td>\n","      <td>0.084629</td>\n","      <td>0.995608</td>\n","      <td>-0.322973</td>\n","      <td>0.999862</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>70.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.023259</td>\n","      <td>0.323456</td>\n","      <td>0.002986</td>\n","      <td>...</td>\n","      <td>0.146055</td>\n","      <td>-0.193611</td>\n","      <td>0.021812</td>\n","      <td>-0.045600</td>\n","      <td>-0.126873</td>\n","      <td>-0.155373</td>\n","      <td>-0.026748</td>\n","      <td>0.996968</td>\n","      <td>-0.111226</td>\n","      <td>0.999883</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>56.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.000354</td>\n","      <td>0.152932</td>\n","      <td>0.241619</td>\n","      <td>0.011037</td>\n","      <td>...</td>\n","      <td>0.104565</td>\n","      <td>-0.132154</td>\n","      <td>0.054910</td>\n","      <td>0.014190</td>\n","      <td>-0.041957</td>\n","      <td>-0.229654</td>\n","      <td>-0.031166</td>\n","      <td>0.998506</td>\n","      <td>-0.178801</td>\n","      <td>0.999936</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>55.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.002057</td>\n","      <td>0.002398</td>\n","      <td>0.132481</td>\n","      <td>0.013952</td>\n","      <td>...</td>\n","      <td>0.134478</td>\n","      <td>-0.053509</td>\n","      <td>0.013095</td>\n","      <td>-0.077658</td>\n","      <td>-0.158625</td>\n","      <td>-0.060205</td>\n","      <td>-0.057243</td>\n","      <td>0.987958</td>\n","      <td>-0.210916</td>\n","      <td>0.999612</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>52.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.000727</td>\n","      <td>0.006700</td>\n","      <td>0.110596</td>\n","      <td>0.004311</td>\n","      <td>...</td>\n","      <td>-0.013853</td>\n","      <td>-0.159797</td>\n","      <td>0.033823</td>\n","      <td>-0.014833</td>\n","      <td>-0.040662</td>\n","      <td>-0.189048</td>\n","      <td>-0.094771</td>\n","      <td>0.986687</td>\n","      <td>-0.243513</td>\n","      <td>0.999846</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>61.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>0.030176</td>\n","      <td>0.289164</td>\n","      <td>0.003144</td>\n","      <td>...</td>\n","      <td>0.045414</td>\n","      <td>-0.076951</td>\n","      <td>0.004563</td>\n","      <td>-0.025448</td>\n","      <td>-0.119935</td>\n","      <td>-0.219748</td>\n","      <td>0.002504</td>\n","      <td>0.995120</td>\n","      <td>-0.231052</td>\n","      <td>0.999886</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>70 rows × 4041 columns</p>\n","</div>"],"text/plain":["    de_0  de_1  de_2  de_3  de_4  de_5      vd_0      vd_1      vd_2  \\\n","0   72.0   1.0   6.0  -1.0   1.0   1.0  0.004280  0.026188  0.263595   \n","1   82.0   1.0   7.0   1.0   1.0   1.0  0.012419  0.003249  0.116324   \n","2   69.0   0.0   7.0   1.0   1.0   1.0  0.000000  0.020264  0.444775   \n","3   70.0   1.0   7.0   1.0   1.0   2.0  0.000000  0.080409  0.463351   \n","4   73.0   0.0   7.0   1.0   1.0   1.0  0.000000  0.086402  0.225096   \n","..   ...   ...   ...   ...   ...   ...       ...       ...       ...   \n","65  70.0   1.0   2.0   1.0   1.0   1.0  0.000000  0.023259  0.323456   \n","66  56.0   1.0   7.0   2.0   1.0   1.0  0.000354  0.152932  0.241619   \n","67  55.0   0.0   2.0   2.0   1.0   1.0  0.002057  0.002398  0.132481   \n","68  52.0   0.0   2.0   2.0   1.0   2.0  0.000727  0.006700  0.110596   \n","69  61.0   1.0   7.0   1.0   1.0   2.0  0.000000  0.030176  0.289164   \n","\n","        vd_3  ...  n_ech_758  n_ech_759  n_ech_760  n_ech_761  n_ech_762  \\\n","0   0.000158  ...   0.008949  -0.101050   0.210530  -0.131684  -0.138715   \n","1   0.000791  ...   0.038696  -0.130439   0.102237   0.025674  -0.135884   \n","2   0.032252  ...   0.039694  -0.141062  -0.005484  -0.036408  -0.018071   \n","3   0.000000  ...   0.003432  -0.111446   0.232872  -0.162072  -0.080258   \n","4   0.003878  ...   0.064002  -0.164014  -0.018618  -0.031740  -0.092928   \n","..       ...  ...        ...        ...        ...        ...        ...   \n","65  0.002986  ...   0.146055  -0.193611   0.021812  -0.045600  -0.126873   \n","66  0.011037  ...   0.104565  -0.132154   0.054910   0.014190  -0.041957   \n","67  0.013952  ...   0.134478  -0.053509   0.013095  -0.077658  -0.158625   \n","68  0.004311  ...  -0.013853  -0.159797   0.033823  -0.014833  -0.040662   \n","69  0.003144  ...   0.045414  -0.076951   0.004563  -0.025448  -0.119935   \n","\n","    n_ech_763  n_ech_764  n_ech_765  n_ech_766  n_ech_767  \n","0   -0.245273   0.038294   0.984190  -0.270763   0.999857  \n","1   -0.178948  -0.064230   0.998583  -0.202867   0.999945  \n","2   -0.271337   0.008223   0.972547  -0.268833   0.999649  \n","3   -0.307298  -0.025907   0.960139  -0.339089   0.999677  \n","4   -0.242412   0.084629   0.995608  -0.322973   0.999862  \n","..        ...        ...        ...        ...        ...  \n","65  -0.155373  -0.026748   0.996968  -0.111226   0.999883  \n","66  -0.229654  -0.031166   0.998506  -0.178801   0.999936  \n","67  -0.060205  -0.057243   0.987958  -0.210916   0.999612  \n","68  -0.189048  -0.094771   0.986687  -0.243513   0.999846  \n","69  -0.219748   0.002504   0.995120  -0.231052   0.999886  \n","\n","[70 rows x 4041 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# pick first 3840 columns from small_x\n","x_df = small_x.iloc[:, :-1]\n","x_df"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([6, 4041])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.FloatTensor(x_df.loc[:5].values)\n","x.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Multimodal BERT Class"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class MultimodalBERT(nn.Module):\n","    \"\"\"\n","    Multimodal BERT\n","    Next Sentence Prediction Model + Masked Language Model\n","    \"\"\"\n","\n","    def __init__(\n","        self, vector_width=4041, seq_len=256, hidden=768, n_layers=12, attn_heads=12, dropout=0.1\n","    ):\n","        \"\"\"\n","        :param bert: BERT model which should be trained\n","        \"\"\"\n","\n","        super().__init__()\n","        self.preprocessor = VectorPreProcessor(vector_width=vector_width, hidden=hidden, seq_len=seq_len)\n","        self.bert = BERT(hidden=hidden, n_layers=n_layers, attn_heads=attn_heads, dropout=dropout)\n","        self.predictor = MultiLabelPredictor(hidden=hidden)\n","\n","    def forward(self, x):  # x: (B, 4041)\n","        x = self.preprocessor(x)  # x: (B, 256, 768)\n","        x = self.bert(x)  # x: (B, seq_len, hidden)\n","        x = self.predictor(x)  # x: (B, 14)\n","        return x  # x: (B, 14)\n","\n","\n","class VectorPreProcessor(nn.Module):\n","    \"\"\"\n","    Preprocesses pretrained vectors before sending to BERT\n","    \"\"\"\n","\n","    def __init__(self, vector_width=4041, hidden=768, seq_len=256):\n","        \"\"\"\n","        :param vector_width: pretrained vector width\n","        :param hidden: BERT model hidden size\n","        :param seq_len: BERT model sequence length\n","        \"\"\"\n","        super().__init__()\n","        self.hidden = hidden\n","        self.seq_len = seq_len\n","        self.linear = nn.Linear(\n","            vector_width, hidden * seq_len\n","        )  # 4041 -> 768*256 = 196,608\n","\n","    def forward(self, x):\n","        print(f\"x.shape from VectorPreProcessor: {x.shape}\")\n","        x = self.linear(x)  # 4041 -> 768*256 = 196,608\n","        return x.view(x.shape[0], self.seq_len, self.hidden)  # 196,608 -> (B, 256, 768)\n","\n","\n","class MultiLabelPredictor(nn.Module):\n","    \"\"\"\n","    14-class multi label classification model on top of BERT\n","    \"\"\"\n","\n","    def __init__(self, hidden):\n","        \"\"\"\n","        :param hidden: BERT model output size\n","        \"\"\"\n","        super().__init__()\n","        self.linear = nn.Linear(hidden, 13)\n","        # self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, x):\n","        print(f\"x.shape from MultiLabelPredictor: {x.shape}\")\n","        return self.linear(x[:, 0])  # x[:, 0]: (B, hidden) -> (B, 14)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### BERT From Scratch"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["h (attn_heads): 12, d_model (hidden): 768, d_model%h = 0\n","x.shape from VectorPreProcessor: torch.Size([6, 4041])\n","x.shape from BERT: torch.Size([6, 256, 768])\n","x.shape from MultiLabelPredictor: torch.Size([6, 256, 768])\n","CPU times: user 12.6 ms, sys: 206 µs, total: 12.8 ms\n","Wall time: 12.7 ms\n","shape of input x: torch.Size([6, 4041])\n","shape of output y: torch.Size([6, 14])\n"]}],"source":["bert_scratch = BERTFromScratch()                           # hidden=768, n_layers=12, attn_heads=12, dropout=0.1\n","mm_bert_1 = MultimodalBERT(bert_scratch).cuda()\n","x = x.cuda()\n","\n","%time y = mm_bert_1(x)\n","\n","print('shape of input x:',x.size())\n","print('shape of output y:', y.size())"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5,428,692,992\n"]}],"source":["print(f\"{torch.cuda.memory_allocated():,}\")"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["MultimodalBERT(\n","  (preprocessor): VectorPreProcessor(\n","    (linear): Linear(in_features=4041, out_features=196608, bias=True)\n","  )\n","  (bert): BERTFromScratch(\n","    (transformer_blocks): ModuleList(\n","      (0-11): 12 x TransformerBlock(\n","        (attention): MultiHeadedAttention(\n","          (linear_layers): ModuleList(\n","            (0-2): 3 x Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n","          (attention): Attention()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation): GELU(approximate='none')\n","        )\n","        (input_sublayer): SublayerConnection(\n","          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (output_sublayer): SublayerConnection(\n","          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (predictor): MultiLabelPredictor(\n","    (linear): Linear(in_features=768, out_features=14, bias=True)\n","  )\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["mm_bert_1.cpu()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 2,249,455,104\n"]}],"source":["print(f\"{torch.cuda.memory_allocated(): ,}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### BERT "]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x.shape from VectorPreProcessor: torch.Size([6, 4041])\n","x.shape from BERT: torch.Size([6, 256, 768])\n","x.shape from MultiLabelPredictor: torch.Size([6, 256, 768])\n","CPU times: user 9.31 ms, sys: 458 µs, total: 9.77 ms\n","Wall time: 9.64 ms\n","shape of input x: torch.Size([6, 4041])\n","shape of output y: torch.Size([6, 14])\n"]}],"source":["bert = BERT()                                       # hidden=768, n_layers=12, attn_heads=12, dropout=0.1\n","mm_bert_2 = MultimodalBERT(bert).to(\"cuda\")                    \n","x = x.to(\"cuda\")\n","\n","%time y = mm_bert_2(x)\n","\n","print('shape of input x:',x.size())\n","print('shape of output y:', y.size())"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def count_parameters(bert, mm_bert):\n","    ### Count parameters and difference between BERT and Multimodal BERT\n","    bert_params = sum([p.nelement() for p in bert.parameters()])\n","    mm_bert_params = sum([p.nelement() for p in mm_bert.parameters()])\n","    print(f\"Total Parameters in BERT: , {bert_params: ,}\")      # nelement(): Alias for numel() \n","    print(f\"Total Parameters in Multimodal BERT: , {mm_bert_params: ,}\")\n","    print(f\"Difference: {mm_bert_params - bert_params: ,} or {mm_bert_params // bert_params}X more params\")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters in BERT: ,  85,054,464\n","Total Parameters in Multimodal BERT: ,  879,754,766\n","Difference:  794,700,302 or 10X more params\n"]}],"source":["count_parameters(bert_scratch, mm_bert_1)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters in BERT: ,  85,054,464\n","Total Parameters in Multimodal BERT: ,  879,754,766\n","Difference:  794,700,302 or 10X more params\n"]}],"source":["count_parameters(bert, mm_bert_2)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PreProcessor:  794,689,536\n"]}],"source":["print(f\"PreProcessor: {sum([p.nelement() for p in mm_bert_2.preprocessor.parameters()]): ,}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Next Steps"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Fusion Options**\n","- Each modality = 1 token - so seq_len = 8 tokens \n","- Proportional scaling\n","\t- Convolutional encoding\n","\t- PCA\n","- Scale and chop\n","\n","**Training**\n","- W&B hyperparam sweep\n","\t- \\# of heads\n","\t- seq length\n","\t- \\# of encoder layers\n","\t- Fusion option\n","- Early stopping"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('../')\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from lemoncake.data import *"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_ds, val_ds, test_ds = get_datasets()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["((43738, 4041), (321, 4041), (991, 4041))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_ds.x.shape, val_ds.x.shape, test_ds.x.shape"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["((43738, 13), (321, 13), (991, 13))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_ds.y.shape, val_ds.y.shape, test_ds.y.shape"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["dls = get_dataloaders({'train': train_ds, 'val': val_ds, 'test': test_ds}, batch_size=32)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_dl, val_dl, test_dl = dls['train'], dls['val'], dls['test']"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["batch = next(iter(train_dl))\n","x, y = batch['x'], batch['y']"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([32, 4041]), torch.Size([32, 13]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x.shape, y.shape"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["mimic_bert = MultimodalBERT(           # vector_width=4041, seq_len=256, dropout=0.1\n","    hidden=384, \n","    n_layers=6, \n","    attn_heads=6\n",").to(\"cuda\")\n","\n","x = x.to(\"cuda\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x.shape from VectorPreProcessor: torch.Size([32, 4041])\n","x.shape from BERT: torch.Size([32, 256, 384])\n","x.shape from MultiLabelPredictor: torch.Size([32, 256, 384])\n","CPU times: user 19.8 ms, sys: 3.97 ms, total: 23.8 ms\n","Wall time: 22.8 ms\n","shape of input x: torch.Size([32, 4041])\n","shape of output y: torch.Size([32, 13])\n"]}],"source":["%time y = mimic_bert(x)\n","\n","print('shape of input x:',x.size())\n","print('shape of output y:', y.size())"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters in BERT: ,  10,646,784\n","Total Parameters in Multimodal BERT: ,  407,996,557\n","Difference:  397,349,773 or 38X more params\n"]}],"source":["count_parameters(mimic_bert.bert, mimic_bert)"]},{"cell_type":"markdown","metadata":{},"source":["# Lightning Models "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from lemoncake.model import *\n","from pytorch_lightning import Trainer, seed_everything"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["model = MultimodalBERT(hidden=192, \n","    n_layers=6, \n","    attn_heads=6)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 42\n","Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["seed_everything(42, workers=True)\n","trainer = Trainer(max_epochs=1, precision=16)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name         | Type                | Params\n","-----------------------------------------------------\n","0 | preprocessor | VectorPreProcessor  | 198 M \n","1 | bert         | BERT                | 2.7 M \n","2 | predictor    | MultiLabelPredictor | 2.5 K \n","-----------------------------------------------------\n","201 M     Trainable params\n","0         Non-trainable params\n","201 M     Total params\n","805.376   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9da52d1f3eb34993b24703f9a5c87288","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38ae5deb523b4cddb639eca412275d7c","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9808cfd307c74d5faf32e7a5d138881c","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=1` reached.\n"]}],"source":["trainer.fit(model, train_dl, val_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rZ9oUDuYsEXq"},"source":["# References\n","- [Sentiment Analysis with BERT and Transformers by Hugging Face using PyTorch and Python](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n","- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n","- [L11 Language Models - Alec Radford (OpenAI)](https://www.youtube.com/watch?v=BnpB3GrpsfM)\n","- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)\n","- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n","- [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n","- [Huggingface Transformers](https://huggingface.co/transformers/)\n","- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n","- [BERT implementation](https://github.com/codertimo/BERT-pytorch)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ai504_12_BERT_sol.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"lemoncake","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d6d780ca4404144b7eca9742de6cab6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_556333e2a3ec4d84b648315544c6ed0e","placeholder":"​","style":"IPY_MODEL_2b6d8d36b27d4eb184451f8e0202b526","value":" 436M/436M [01:05&lt;00:00, 6.62MB/s]"}},"2b6d8d36b27d4eb184451f8e0202b526":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f5394903cde40f3991f393fabeb589a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"43e17641903c40f69ac1dc46a9f64d2b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"4869b620dea848eaab537cb4fa65bd46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_698fa86aa1744c3c88e26054160319d3","max":433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43e17641903c40f69ac1dc46a9f64d2b","value":433}},"4ddc8a91880c49a7bb5b6dc31e8a312e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70e78ca012ae43e59a5368ced3355054","IPY_MODEL_0d6d780ca4404144b7eca9742de6cab6"],"layout":"IPY_MODEL_c2e94c0191a143d5ace71bc67690bedd"}},"556333e2a3ec4d84b648315544c6ed0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"605311765a094c4c8b5e61c5a4ec7d2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4869b620dea848eaab537cb4fa65bd46","IPY_MODEL_e3fe93d78ab14b53ba55e21baca50c55"],"layout":"IPY_MODEL_9703efd4036f48aab09bb784a80a97d0"}},"698fa86aa1744c3c88e26054160319d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e71c68e8356499e96c9ea5031f13d3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70e78ca012ae43e59a5368ced3355054":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_86673f3f83964f47817e6dc08cd4e549","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f5394903cde40f3991f393fabeb589a","value":435779157}},"86673f3f83964f47817e6dc08cd4e549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"933aa95ba4e640c09865724358d934cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9703efd4036f48aab09bb784a80a97d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e94c0191a143d5ace71bc67690bedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3fe93d78ab14b53ba55e21baca50c55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e71c68e8356499e96c9ea5031f13d3c","placeholder":"​","style":"IPY_MODEL_933aa95ba4e640c09865724358d934cc","value":" 433/433 [01:06&lt;00:00, 6.54B/s]"}}}}},"nbformat":4,"nbformat_minor":0}
